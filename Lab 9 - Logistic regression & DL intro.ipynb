{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9:  Logistic Regression & DL intro\n",
    "\n",
    "## A: *Logistic regression* \n",
    "## B: [*Convolutional neural network for image classification*](#partB)\n",
    "\n",
    "### [Haiping Lu](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  [COM4509/6509 MLAI2019](https://github.com/maalvarezl/MLAI)\n",
    "\n",
    "### Part 9A: based on the [one neuron](https://github.com/cbernet/maldives/tree/master/one_neuron) notebooks by  [Colin Bernet](https://github.com/cbernet) \n",
    "### Part 9B: based on  [the CIFAR10 Pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) and [CNN notebook from Lisa Zhang](https://www.cs.toronto.edu/~lczhang/360/lec/w04/convnet.html), part of my  [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) project \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Logistic Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. The sigmoid function \n",
    "\n",
    "The **sigmoid** or **logistic function** is essential in binary classification problems. It is expressed as\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "and here is what it looks like in 1D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "# the bias: \n",
    "b = 0\n",
    "# the weight: \n",
    "w = 1\n",
    "\n",
    "def sigmoid(x1):\n",
    "    # z is a linear function of x1\n",
    "    z = w*x1 + b\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "# create an array of evenly spaced values\n",
    "linx = np.linspace(-10,10,51)\n",
    "plt.plot(linx, sigmoid(linx))\n",
    "b=5\n",
    "plt.plot(linx, sigmoid(linx))\n",
    "w=5\n",
    "plt.plot(linx, sigmoid(linx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this function in more details:\n",
    "\n",
    "* when $z$ goes to infinity, $e^{-z}$ goes to zero, and $\\sigma (z)$ goes to one.\n",
    "* when $z$ goes to minus infinity, $e^{-z}$ goes to infinity, and $\\sigma (z)$ goes to zero.\n",
    "* $\\sigma(0) = 0.5$, since $e^0=1$.\n",
    "\n",
    "It is important to note that the sigmoid is bound between 0 and 1, like a probability. And actually, in binary classification problems, the probability for an example to belong to a given category is produced by a sigmoid function. To classify our examples, we can simply use the output of the sigmoid: A given unknown example with value $x$ will be classified to category 1 if $\\sigma(z) > 0.5$, and to category 0 otherwise. \n",
    "\n",
    "**Excercise**: Now you can go back to the cell above, and play a bit with the `b` and `w` parameters, redoing the plot everytime you change one of these parameters. \n",
    "\n",
    "* $b$ is the **bias**. Changing the bias simply moves the sigmoid along the horizontal axis. For example, if you choose $b=1$ and $w=1$, then $z = wx + b = 0$ at $x=-1$, and that's where the sigmoid will be equal to 0.5\n",
    "* $w$ is the **weight** of variable $x$. If you increase it, the sigmoid evolves faster as a function of $x$ and gets sharper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Logistic regression as the simplest neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build simplest neural network to classify our examples:\n",
    "\n",
    "* Each example has one variable, so we need 1 input node on the input layer\n",
    "* We're not going to use any hidden layer, as that would complicate the network \n",
    "* We have two categories, so the output of the network should be a single value between 0 and 1, which is the estimated probability $p$ for an example to belong to category 1. Then, the probability to belong to category 0 is simply $1-p$. Therefore, we should have a single output neuron, the only neuron in the network.\n",
    "\n",
    "The sigmoid function can be used in the output neuron. Indeed, it spits out a value between 0 and 1, and can be used as a classification probability as we have seen in the previous section.\n",
    "\n",
    "We can represent our network in the following way:\n",
    "\n",
    "![Neural network with 1 neuron](https://github.com/cbernet/maldives/raw/master/images/one_neuron.png)\n",
    "\n",
    "In the output neuron: \n",
    "\n",
    "* the first box performs a change of variable and computes the **weighted input** $z$ of the neuron\n",
    "* the second box applies the **activation function** to the weighted input. Here, we choose the sigmoid $\\sigma (z) = 1/(1+e^{-z})$ as an activation function\n",
    "\n",
    "This simple network has only 2 tunable parameters, the weight $w$ and the bias $b$, both used in the first box. We see in particular that when the bias is very large, the neuron will **always be activated**, whatever the input. On the contrary, for very negative biases, the neuron is **dead**. \n",
    "\n",
    "We can write the output simply as a function of $x$, \n",
    "\n",
    "$$f(x) = \\sigma(z) = \\sigma(wx+b)$$\n",
    "This is exactly the **logistic regression** classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Classifying 2D dataset with logistic regression\n",
    "\n",
    "Let's create a sample of examples with two values x1 and x2, with two categories. \n",
    "For category 0, the underlying probability distribution is a 2D Gaussian centered on (0,0), with width = 1 along both directions. For category 1, the Gaussian is centered on (2,2). We assign label 0 to category 0, and label 1 to category 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "Let's create a sample of examples with two values x1 and x2, with two categories. \n",
    "For category 0, the underlying probability distribution is a 2D Gaussian centered on (0,0), with width = 1 along both directions. For category 1, the Gaussian is centered on (2,2). We assign label 0 to category 0, and label 1 to category 1. Check out the [documentation for Gaussian data generation](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.multivariate_normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = np.random.multivariate_normal\n",
    "# Number of samples \n",
    "nSamples = 500\n",
    "# (unit) variance:\n",
    "s2 = 1\n",
    "# below, we provide the coordinates of the mean as \n",
    "# a first argument, and then the covariance matrix\n",
    "# we generate nexamples examples for each category\n",
    "sgx0 = normal([0.,0.], [[s2, 0.], [0.,s2]], nSamples)\n",
    "sgx1 = normal([2.,2.], [[s2, 0.], [0.,s2]], nSamples)\n",
    "# setting the labels for each category\n",
    "sgy0 = np.zeros((nSamples,))\n",
    "sgy1 = np.ones((nSamples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a scatter plot for the examples in the two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sgx0[:,0], sgx0[:,1], alpha=0.5)\n",
    "plt.scatter(sgx1[:,0], sgx1[:,1], alpha=0.5)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to train a logistic regression to classify (x1,x2) points in one of the two categories depending on the values of x1 and x2. We form the dataset by concatenating the arrays of points, and also the arrays of labels for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgx = np.concatenate((sgx0, sgx1))\n",
    "sgy = np.concatenate((sgy0, sgy1))\n",
    "\n",
    "print(sgx.shape, sgy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D sigmoid\n",
    "\n",
    "In 2D, the expression of the sigmoid remains the same, but $z$ is now a function of the two variables $x_1$ and $x_2$, \n",
    "\n",
    "$$z=w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "And here is the code for the 2D sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "# bias: \n",
    "b = 0\n",
    "# x1 weight: \n",
    "w1 = 1\n",
    "# x2 weight:\n",
    "w2 = 2\n",
    "\n",
    "def sigmoid_2d(x1, x2):\n",
    "    # z is a linear function of x1 and x2\n",
    "    z = w1*x1 + w2*x2 + b\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this function looks like, we can make a 2D plot, with x1 on the horizontal axis, x2 on the vertical axis, and the value of the sigmoid represented as a color for each (x1, x2) coordinate. To do that, we will create an array of evenly spaced values along x1, and another array along x2. Taken together, these arrays will allow us to map the (x1,x2) plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax, npoints = (-6,6,51)\n",
    "linx1 = np.linspace(xmin,xmax,npoints)\n",
    "# no need for a new array, we just reuse the one we have with another name: \n",
    "linx2 = linx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a **meshgrid** from these arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridx1, gridx2 = np.meshgrid(np.linspace(xmin,xmax,npoints), np.linspace(xmin,xmax,npoints))\n",
    "print(gridx1.shape, gridx2.shape)\n",
    "print('gridx1:')\n",
    "print(gridx1) \n",
    "print('gridx2')\n",
    "print(gridx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you take the first line in both arrays, and scan the values on this line, you get: `(-6,-6), (-5.76, -6), (-5.52, -6)`... So we are scanning the x1 coordinates sequentially at the bottom of the plot. If you take the second line, you get: `(-6, -5.76), (-5.76, -5.76), (-5.52, -5.76)` ... : we are scanning the second line at the bottom of the plot, after moving up in x2 from one step. \n",
    "\n",
    "Scanning the full grid, you would scan the whole plot sequentially. \n",
    "\n",
    "Now we need to compute the value of the sigmoid for each pair (x1,x2) in the grid. That's very easy to do with the output of `meshgrid`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sigmoid_2d(gridx1, gridx2)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls the `sigmoid_2d` function to each pair `(x1,y2)` taken from the `gridx1` and `gridx2` arrays so that we can plot our sigmoid in 2D: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(gridx1, gridx2, z)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D sigmoid has the same kind of rising edge as the 1D sigmoid, but in 2D. \n",
    "With the parameters defined above: \n",
    "\n",
    "* The **weight** of $x_2$ is twice larger than the weight of $x_1$, so the sigmoid evolves twice faster as a function of $x_2$. If you set one of the weights to zero, can you guess what will happen? You can test by editing the function `sigmoid_2d`, before re-executing the above cells. \n",
    "* The separation boundary, which occurs for $z=0$, is a straight line with equation $w_1 x_1 + w_2 x_2 + b = 0$. Or equivalently: \n",
    "\n",
    "$$x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2} = -0.5 x_1$$\n",
    "\n",
    "Please verify on the plot above that this equation is indeed the one describing the separation boundary. \n",
    "\n",
    "Note that if you prefer, you can plot the sigmoid in 3D like this:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(gridx1,gridx2,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: change the parameters to observe how the 2D sigmoid changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on the 2D data\n",
    "\n",
    "Let's now train a logistic regression to separate the two classes of examples. The goal of the training will be to use the existing examples to find the optimal values for the parameters $w_1, w_2, b$. \n",
    "\n",
    "We take the logistic regression algorithm from scikit-learn. \n",
    "Here, the logistic regression is used with the `lbfgs` solver. LBFGS is the minimization method used to find the best parameters. It is similar to [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs')  #clf: classifier\n",
    "clf.fit(sgx, sgy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression has been fitted (trained) to the data. Now, we can use it to predict the probability for any given (x1,x2) point to belong to category 1.\n",
    "\n",
    "We would like to plot this probability in 2D as a function of x1 and x2. To do that, we need to use the `clf.predict_proba` method which takes a 2D array of shape `(n_points, 2)`. The first dimension indexes the points, and the second one contains the values of x1 and x2. Again, we use our grid to map the (x1,x2) plane. But the gridx1 and gridx2 arrays defined above contain disconnected values of x1 and x2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gridx1.shape, gridx2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is a 2D array of shape `(n_points, 2)`, not two 2D arrays of shape (51, 51)... \n",
    "So we need to reshape these arrays. First, we will flatten the gridx1 and gridx2 arrays so that all their values appear sequentially in a 1D array. Here is a small example to show how flatten works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0, 1], [2, 3]])\n",
    "print(a) \n",
    "print('flat array:', a.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will stitch the two 1D arrays together in two columns with np.c_ like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[4, 5], [6, 7]])\n",
    "print(a.flatten())\n",
    "print(b.flatten())\n",
    "c = np.c_[a.flatten(), b.flatten()]\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array has exactly the shape expected by `clf.predict_proba`: a list of samples with two values. So let's do the same with our meshgrid, and let's compute the probabilities for all (x1,x2) pairs in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.c_[gridx1.flatten(), gridx2.flatten()]\n",
    "prob = clf.predict_proba(grid)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prob does not have the right shape to be plotted. Below, we will use a gridx1 and a gridx2 array with shapes (51,51). The shape of the prob array must also be (51,51), as the plotting method will simply map each (x1,x2) pair to a probability. So we need to reshape our probability array to shape (51,51). Reshaping works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array([0,1,2,3])\n",
    "print(d)\n",
    "print('reshaped to (2,2):')\n",
    "print(d.reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (!) we can do our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that prob[:,1] returns, for all exemples, the probability p to belong to category 1. \n",
    "# prob[:,0] would return the probability to belong to category 0 (which is 1-p)\n",
    "plt.pcolor(gridx1,gridx2,prob[:,1].reshape(npoints,npoints))\n",
    "plt.colorbar()\n",
    "plt.scatter(sgx0[:,0], sgx0[:,1], alpha=0.5)\n",
    "plt.scatter(sgx1[:,0], sgx1[:,1], alpha=0.5)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the logistic regression is able to separate these two classes well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='partB'></a>Part B: Convolutional NN for image classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
